{"Event":"SparkListenerLogStart","Spark Version":"1.5.2"}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"driver","Host":"198.202.113.139","Port":45003},"Maximum Memory":33339683635,"Timestamp":1487698426087}
{"Event":"SparkListenerEnvironmentUpdate","JVM Information":{"Java Home":"/usr/java/jdk1.8.0_60/jre","Java Version":"1.8.0_60 (Oracle Corporation)","Scala Version":"version 2.10.4"},"Spark Properties":{"spark.driver.host":"198.202.113.139","spark.serializer.objectStreamReset":"100","spark.eventLog.enabled":"true","spark.driver.maxResultSize":"30g","spark.ui.port":"4045","spark.driver.port":"35588","spark.rdd.compress":"True","spark.app.name":"PythonLeafletFinder","spark.scheduler.mode":"FIFO","spark.driver.memory":"60g","spark.files":"file:/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/unit.000000/leaflet-finder-parallel-cc.py","spark.executor.id":"driver","spark.submit.deployMode":"client","spark.master":"spark://comet-29-01.sdsc.edu:7077","spark.executor.memory":"60g","spark.eventLog.dir":"./","spark.fileserver.uri":"http://198.202.113.139:32793","spark.externalBlockStore.folderName":"spark-38a511e5-cb70-4685-9fc1-5752d5ae74fe","spark.app.id":"app-20170221093345-0000"},"System Properties":{"java.io.tmpdir":"/tmp","line.separator":"\n","path.separator":":","sun.management.compiler":"HotSpot 64-Bit Tiered Compilers","SPARK_SUBMIT":"true","sun.cpu.endian":"little","java.specification.version":"1.8","java.vm.specification.name":"Java Virtual Machine Specification","java.vendor":"Oracle Corporation","java.vm.specification.version":"1.8","user.home":"/home/iparask","file.encoding.pkg":"sun.io","sun.nio.ch.bugLevel":"","sun.arch.data.model":"64","sun.boot.library.path":"/usr/java/jdk1.8.0_60/jre/lib/amd64","user.dir":"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/unit.000000","java.library.path":"/opt/python/lib:/opt/gnu/gcc/lib64:/opt/gnu/gmp/lib:/opt/gnu/mpfr/lib:/opt/gnu/mpc/lib:/opt/gnu/lib:/opt/gnu/lib64:/opt/mvapich2/intel/ib/lib:/opt/intel/composer_xe_2013_sp1.2.144/compiler/lib/intel64:/opt/intel/composer_xe_2013_sp1.2.144/mpirt/lib/intel64:/opt/intel/composer_xe_2013_sp1.2.144/ipp/../compiler/lib/intel64:/opt/intel/composer_xe_2013_sp1.2.144/ipp/lib/intel64:/opt/intel/composer_xe_2013_sp1.2.144/compiler/lib/intel64:/opt/intel/composer_xe_2013_sp1.2.144/mkl/lib/intel64:/opt/intel/composer_xe_2013_sp1.2.144/tbb/lib/intel64/gcc4.4:/opt/sdsc/lib:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib","sun.cpu.isalist":"","os.arch":"amd64","java.vm.version":"25.60-b23","java.endorsed.dirs":"/usr/java/jdk1.8.0_60/jre/lib/endorsed","java.runtime.version":"1.8.0_60-b27","java.vm.info":"mixed mode","java.ext.dirs":"/usr/java/jdk1.8.0_60/jre/lib/ext:/usr/java/packages/lib/ext","java.runtime.name":"Java(TM) SE Runtime Environment","file.separator":"/","java.class.version":"52.0","java.specification.name":"Java Platform API Specification","sun.boot.class.path":"/usr/java/jdk1.8.0_60/jre/lib/resources.jar:/usr/java/jdk1.8.0_60/jre/lib/rt.jar:/usr/java/jdk1.8.0_60/jre/lib/sunrsasign.jar:/usr/java/jdk1.8.0_60/jre/lib/jsse.jar:/usr/java/jdk1.8.0_60/jre/lib/jce.jar:/usr/java/jdk1.8.0_60/jre/lib/charsets.jar:/usr/java/jdk1.8.0_60/jre/lib/jfr.jar:/usr/java/jdk1.8.0_60/jre/classes","file.encoding":"UTF-8","user.timezone":"America/Los_Angeles","java.specification.vendor":"Oracle Corporation","sun.java.launcher":"SUN_STANDARD","os.version":"2.6.32-642.13.1.el6.x86_64","sun.os.patch.level":"unknown","java.vm.specification.vendor":"Oracle Corporation","user.country":"US","sun.jnu.encoding":"UTF-8","user.language":"en","java.vendor.url":"http://java.oracle.com/","java.awt.printerjob":"sun.print.PSPrinterJob","java.awt.graphicsenv":"sun.awt.X11GraphicsEnvironment","awt.toolkit":"sun.awt.X11.XToolkit","os.name":"Linux","java.vm.vendor":"Oracle Corporation","java.vendor.url.bug":"http://bugreport.sun.com/bugreport/","user.name":"iparask","java.vm.name":"Java HotSpot(TM) 64-Bit Server VM","sun.java.command":"org.apache.spark.deploy.SparkSubmit --conf spark.driver.memory=60g --conf spark.driver.maxResultSize=30g --conf spark.eventLog.enabled=true --conf spark.eventLog.dir=./ --conf spark.ui.port=4045 --executor-memory 60g leaflet-finder-parallel-cc.py 32768 atom_pos_262K.npy","java.home":"/usr/java/jdk1.8.0_60/jre","java.version":"1.8.0_60","sun.io.unicode.encoding":"UnicodeLittle"},"Classpath Entries":{"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/lib/datanucleus-core-3.2.10.jar":"System Classpath","/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/conf/":"System Classpath","/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/lib/datanucleus-rdbms-3.2.9.jar":"System Classpath","/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/lib/datanucleus-api-jdo-3.2.6.jar":"System Classpath","http://198.202.113.139:32793/files/leaflet-finder-parallel-cc.py":"Added By User","/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/lib/spark-assembly-1.5.2-hadoop2.6.0.jar":"System Classpath"}}
{"Event":"SparkListenerApplicationStart","App Name":"PythonLeafletFinder","App ID":"app-20170221093345-0000","Timestamp":1487698424676,"User":"iparask"}
{"Event":"SparkListenerJobStart","Job ID":0,"Submission Time":1487698426614,"Stage Infos":[{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"reduce at /home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/unit.000000/leaflet-finder-parallel-cc.py:79","Number of Tasks":64,"RDD Info":[{"RDD ID":1,"Name":"PythonRDD","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":64,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":64,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"","Accumulables":[]}],"Stage IDs":[0],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"1\",\"name\":\"collect\"}","callSite.short":"reduce at /home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/unit.000000/leaflet-finder-parallel-cc.py:79"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"reduce at /home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/unit.000000/leaflet-finder-parallel-cc.py:79","Number of Tasks":64,"RDD Info":[{"RDD ID":1,"Name":"PythonRDD","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":64,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":64,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"1\",\"name\":\"collect\"}","callSite.short":"reduce at /home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/unit.000000/leaflet-finder-parallel-cc.py:79"}}
{"Event":"SparkListenerExecutorAdded","Timestamp":1487698427715,"Executor ID":"1","Executor Info":{"Host":"198.202.113.152","Total Cores":24,"Log Urls":{"stdout":"http://198.202.113.152:8081/logPage/?appId=app-20170221093345-0000&executorId=1&logType=stdout","stderr":"http://198.202.113.152:8081/logPage/?appId=app-20170221093345-0000&executorId=1&logType=stderr"}}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":0,"Index":0,"Attempt":0,"Launch Time":1487698427717,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":1,"Index":1,"Attempt":0,"Launch Time":1487698427729,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":2,"Index":2,"Attempt":0,"Launch Time":1487698427731,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":3,"Index":3,"Attempt":0,"Launch Time":1487698427733,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":4,"Index":4,"Attempt":0,"Launch Time":1487698427735,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":5,"Index":5,"Attempt":0,"Launch Time":1487698427737,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":6,"Index":6,"Attempt":0,"Launch Time":1487698427738,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":7,"Index":7,"Attempt":0,"Launch Time":1487698427740,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":8,"Index":8,"Attempt":0,"Launch Time":1487698427742,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":9,"Index":9,"Attempt":0,"Launch Time":1487698427743,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":10,"Index":10,"Attempt":0,"Launch Time":1487698427745,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":11,"Index":11,"Attempt":0,"Launch Time":1487698427747,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":12,"Index":12,"Attempt":0,"Launch Time":1487698427749,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":13,"Index":13,"Attempt":0,"Launch Time":1487698427750,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":14,"Index":14,"Attempt":0,"Launch Time":1487698427752,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":15,"Index":15,"Attempt":0,"Launch Time":1487698427753,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":16,"Index":16,"Attempt":0,"Launch Time":1487698427755,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":17,"Index":17,"Attempt":0,"Launch Time":1487698427757,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":18,"Index":18,"Attempt":0,"Launch Time":1487698427759,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":19,"Index":19,"Attempt":0,"Launch Time":1487698427760,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":20,"Index":20,"Attempt":0,"Launch Time":1487698427762,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":21,"Index":21,"Attempt":0,"Launch Time":1487698427763,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":22,"Index":22,"Attempt":0,"Launch Time":1487698427765,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":23,"Index":23,"Attempt":0,"Launch Time":1487698427766,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerExecutorAdded","Timestamp":1487698427800,"Executor ID":"0","Executor Info":{"Host":"198.202.113.154","Total Cores":24,"Log Urls":{"stdout":"http://198.202.113.154:8081/logPage/?appId=app-20170221093345-0000&executorId=0&logType=stdout","stderr":"http://198.202.113.154:8081/logPage/?appId=app-20170221093345-0000&executorId=0&logType=stderr"}}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":24,"Index":24,"Attempt":0,"Launch Time":1487698427801,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":25,"Index":25,"Attempt":0,"Launch Time":1487698427802,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":26,"Index":26,"Attempt":0,"Launch Time":1487698427804,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":27,"Index":27,"Attempt":0,"Launch Time":1487698427806,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":28,"Index":28,"Attempt":0,"Launch Time":1487698427807,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":29,"Index":29,"Attempt":0,"Launch Time":1487698427809,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":30,"Index":30,"Attempt":0,"Launch Time":1487698427811,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":31,"Index":31,"Attempt":0,"Launch Time":1487698427813,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":32,"Index":32,"Attempt":0,"Launch Time":1487698427814,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":33,"Index":33,"Attempt":0,"Launch Time":1487698427816,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":34,"Index":34,"Attempt":0,"Launch Time":1487698427818,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":35,"Index":35,"Attempt":0,"Launch Time":1487698427819,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":36,"Index":36,"Attempt":0,"Launch Time":1487698427821,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":37,"Index":37,"Attempt":0,"Launch Time":1487698427823,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":38,"Index":38,"Attempt":0,"Launch Time":1487698427824,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":39,"Index":39,"Attempt":0,"Launch Time":1487698427826,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":40,"Index":40,"Attempt":0,"Launch Time":1487698427827,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"1","Host":"198.202.113.152","Port":39182},"Maximum Memory":33339683635,"Timestamp":1487698427829}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":41,"Index":41,"Attempt":0,"Launch Time":1487698427829,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":42,"Index":42,"Attempt":0,"Launch Time":1487698427830,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":43,"Index":43,"Attempt":0,"Launch Time":1487698427832,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":44,"Index":44,"Attempt":0,"Launch Time":1487698427834,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":45,"Index":45,"Attempt":0,"Launch Time":1487698427835,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":46,"Index":46,"Attempt":0,"Launch Time":1487698427837,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":47,"Index":47,"Attempt":0,"Launch Time":1487698427838,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerExecutorAdded","Timestamp":1487698427868,"Executor ID":"2","Executor Info":{"Host":"198.202.113.161","Total Cores":24,"Log Urls":{"stdout":"http://198.202.113.161:8081/logPage/?appId=app-20170221093345-0000&executorId=2&logType=stdout","stderr":"http://198.202.113.161:8081/logPage/?appId=app-20170221093345-0000&executorId=2&logType=stderr"}}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":48,"Index":48,"Attempt":0,"Launch Time":1487698427868,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":49,"Index":49,"Attempt":0,"Launch Time":1487698427870,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":50,"Index":50,"Attempt":0,"Launch Time":1487698427872,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":51,"Index":51,"Attempt":0,"Launch Time":1487698427874,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":52,"Index":52,"Attempt":0,"Launch Time":1487698427876,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":53,"Index":53,"Attempt":0,"Launch Time":1487698427877,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":54,"Index":54,"Attempt":0,"Launch Time":1487698427879,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":55,"Index":55,"Attempt":0,"Launch Time":1487698427881,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":56,"Index":56,"Attempt":0,"Launch Time":1487698427882,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":57,"Index":57,"Attempt":0,"Launch Time":1487698427884,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":58,"Index":58,"Attempt":0,"Launch Time":1487698427886,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":59,"Index":59,"Attempt":0,"Launch Time":1487698427888,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":60,"Index":60,"Attempt":0,"Launch Time":1487698427889,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":61,"Index":61,"Attempt":0,"Launch Time":1487698427891,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":62,"Index":62,"Attempt":0,"Launch Time":1487698427893,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":63,"Index":63,"Attempt":0,"Launch Time":1487698427894,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"0","Host":"198.202.113.154","Port":41007},"Maximum Memory":33339683635,"Timestamp":1487698427915}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"2","Host":"198.202.113.161","Port":34829},"Maximum Memory":33339683635,"Timestamp":1487698428004}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"org.apache.spark.SparkException","Description":"Python worker exited unexpectedly (crashed)","Stack Trace":[{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"read","File Name":"PythonRDD.scala","Line Number":203},{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"<init>","File Name":"PythonRDD.scala","Line Number":207},{"Declaring Class":"org.apache.spark.api.python.PythonRunner","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":125},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":70},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":300},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":264},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":66},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":88},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":214},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1142},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":617},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":745}],"Full Stack Trace":"org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:203)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:139)\n\t... 11 more\n","Metrics":{"Host Name":"198.202.113.152","Executor Deserialize Time":0,"Executor Run Time":7456,"Result Size":0,"JVM GC Time":53,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}},"Task Info":{"Task ID":5,"Index":5,"Attempt":0,"Launch Time":1487698427737,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1487698435521,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"198.202.113.152","Executor Deserialize Time":0,"Executor Run Time":7456,"Result Size":0,"JVM GC Time":53,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":64,"Index":5,"Attempt":1,"Launch Time":1487698435523,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"org.apache.spark.SparkException","Description":"Python worker exited unexpectedly (crashed)","Stack Trace":[{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"read","File Name":"PythonRDD.scala","Line Number":203},{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"<init>","File Name":"PythonRDD.scala","Line Number":207},{"Declaring Class":"org.apache.spark.api.python.PythonRunner","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":125},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":70},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":300},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":264},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":66},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":88},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":214},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1142},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":617},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":745}],"Full Stack Trace":"org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:203)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:139)\n\t... 11 more\n","Metrics":{"Host Name":"198.202.113.154","Executor Deserialize Time":0,"Executor Run Time":7559,"Result Size":0,"JVM GC Time":47,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}},"Task Info":{"Task ID":33,"Index":33,"Attempt":0,"Launch Time":1487698427816,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1487698435622,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"198.202.113.154","Executor Deserialize Time":0,"Executor Run Time":7559,"Result Size":0,"JVM GC Time":47,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":65,"Index":33,"Attempt":1,"Launch Time":1487698435623,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.net.SocketException","Description":"Connection reset","Stack Trace":[{"Declaring Class":"java.net.SocketInputStream","Method Name":"read","File Name":"SocketInputStream.java","Line Number":209},{"Declaring Class":"java.net.SocketInputStream","Method Name":"read","File Name":"SocketInputStream.java","Line Number":141},{"Declaring Class":"java.io.BufferedInputStream","Method Name":"fill","File Name":"BufferedInputStream.java","Line Number":246},{"Declaring Class":"java.io.BufferedInputStream","Method Name":"read","File Name":"BufferedInputStream.java","Line Number":265},{"Declaring Class":"java.io.DataInputStream","Method Name":"readInt","File Name":"DataInputStream.java","Line Number":387},{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"read","File Name":"PythonRDD.scala","Line Number":139},{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"<init>","File Name":"PythonRDD.scala","Line Number":207},{"Declaring Class":"org.apache.spark.api.python.PythonRunner","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":125},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":70},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":300},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":264},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":66},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":88},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":214},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1142},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":617},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":745}],"Full Stack Trace":"java.net.SocketException: Connection reset\n\tat java.net.SocketInputStream.read(SocketInputStream.java:209)\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:139)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n","Metrics":{"Host Name":"198.202.113.152","Executor Deserialize Time":0,"Executor Run Time":7941,"Result Size":0,"JVM GC Time":53,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}},"Task Info":{"Task ID":3,"Index":3,"Attempt":0,"Launch Time":1487698427733,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1487698435937,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"198.202.113.152","Executor Deserialize Time":0,"Executor Run Time":7941,"Result Size":0,"JVM GC Time":53,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":66,"Index":3,"Attempt":1,"Launch Time":1487698435937,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"org.apache.spark.SparkException","Description":"Python worker exited unexpectedly (crashed)","Stack Trace":[{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"read","File Name":"PythonRDD.scala","Line Number":203},{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"<init>","File Name":"PythonRDD.scala","Line Number":207},{"Declaring Class":"org.apache.spark.api.python.PythonRunner","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":125},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":70},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":300},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":264},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":66},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":88},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":214},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1142},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":617},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":745}],"Full Stack Trace":"org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:203)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:139)\n\t... 11 more\n","Metrics":{"Host Name":"198.202.113.154","Executor Deserialize Time":0,"Executor Run Time":8109,"Result Size":0,"JVM GC Time":47,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}},"Task Info":{"Task ID":24,"Index":24,"Attempt":0,"Launch Time":1487698427801,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1487698436147,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"198.202.113.154","Executor Deserialize Time":0,"Executor Run Time":8109,"Result Size":0,"JVM GC Time":47,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":67,"Index":24,"Attempt":1,"Launch Time":1487698436147,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"org.apache.spark.SparkException","Description":"Python worker exited unexpectedly (crashed)","Stack Trace":[{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"read","File Name":"PythonRDD.scala","Line Number":203},{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"<init>","File Name":"PythonRDD.scala","Line Number":207},{"Declaring Class":"org.apache.spark.api.python.PythonRunner","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":125},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":70},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":300},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":264},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":66},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":88},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":214},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1142},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":617},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":745}],"Full Stack Trace":"org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:203)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:139)\n\t... 11 more\n","Metrics":{"Host Name":"198.202.113.152","Executor Deserialize Time":0,"Executor Run Time":8478,"Result Size":0,"JVM GC Time":53,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}},"Task Info":{"Task ID":2,"Index":2,"Attempt":0,"Launch Time":1487698427731,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1487698436471,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"198.202.113.152","Executor Deserialize Time":0,"Executor Run Time":8478,"Result Size":0,"JVM GC Time":53,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":68,"Index":2,"Attempt":1,"Launch Time":1487698436471,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"org.apache.spark.SparkException","Description":"Python worker exited unexpectedly (crashed)","Stack Trace":[{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"read","File Name":"PythonRDD.scala","Line Number":203},{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"<init>","File Name":"PythonRDD.scala","Line Number":207},{"Declaring Class":"org.apache.spark.api.python.PythonRunner","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":125},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":70},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":300},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":264},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":66},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":88},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":214},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1142},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":617},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":745}],"Full Stack Trace":"org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:203)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:139)\n\t... 11 more\n","Metrics":{"Host Name":"198.202.113.154","Executor Deserialize Time":0,"Executor Run Time":8684,"Result Size":0,"JVM GC Time":47,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}},"Task Info":{"Task ID":36,"Index":36,"Attempt":0,"Launch Time":1487698427821,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1487698436722,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"198.202.113.154","Executor Deserialize Time":0,"Executor Run Time":8684,"Result Size":0,"JVM GC Time":47,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":69,"Index":36,"Attempt":1,"Launch Time":1487698436722,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"org.apache.spark.api.python.PythonException","Description":"Traceback (most recent call last):\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/rdd.py\", line 794, in func\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/unit.000000/leaflet-finder-parallel-cc.py\", line 20, in find_partial_connected_components\n    distances = (cdist(window[0],window[1])<cutoff)  # check indexes\n  File \"/home/iparask/radical.pilot.sandbox/ve_comet/lib/python2.7/site-packages/scipy/spatial/distance.py\", line 2037, in cdist\n    dm = np.zeros((mA, mB), dtype=np.double)\nMemoryError\n","Stack Trace":[{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"read","File Name":"PythonRDD.scala","Line Number":166},{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"<init>","File Name":"PythonRDD.scala","Line Number":207},{"Declaring Class":"org.apache.spark.api.python.PythonRunner","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":125},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":70},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":300},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":264},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":66},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":88},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":214},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1142},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":617},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":745}],"Full Stack Trace":"org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/rdd.py\", line 794, in func\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/unit.000000/leaflet-finder-parallel-cc.py\", line 20, in find_partial_connected_components\n    distances = (cdist(window[0],window[1])<cutoff)  # check indexes\n  File \"/home/iparask/radical.pilot.sandbox/ve_comet/lib/python2.7/site-packages/scipy/spatial/distance.py\", line 2037, in cdist\n    dm = np.zeros((mA, mB), dtype=np.double)\nMemoryError\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n","Metrics":{"Host Name":"198.202.113.161","Executor Deserialize Time":0,"Executor Run Time":418,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}},"Task Info":{"Task ID":68,"Index":2,"Attempt":1,"Launch Time":1487698436471,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1487698436925,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"198.202.113.161","Executor Deserialize Time":0,"Executor Run Time":418,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":70,"Index":2,"Attempt":2,"Launch Time":1487698436925,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"org.apache.spark.SparkException","Description":"Python worker exited unexpectedly (crashed)","Stack Trace":[{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"read","File Name":"PythonRDD.scala","Line Number":203},{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"<init>","File Name":"PythonRDD.scala","Line Number":207},{"Declaring Class":"org.apache.spark.api.python.PythonRunner","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":125},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":70},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":300},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":264},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":66},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":88},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":214},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1142},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":617},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":745}],"Full Stack Trace":"org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:203)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:139)\n\t... 11 more\n","Metrics":{"Host Name":"198.202.113.152","Executor Deserialize Time":0,"Executor Run Time":9036,"Result Size":0,"JVM GC Time":53,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}},"Task Info":{"Task ID":9,"Index":9,"Attempt":0,"Launch Time":1487698427743,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1487698437028,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"198.202.113.152","Executor Deserialize Time":0,"Executor Run Time":9036,"Result Size":0,"JVM GC Time":53,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":71,"Index":9,"Attempt":1,"Launch Time":1487698437029,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"org.apache.spark.api.python.PythonException","Description":"Traceback (most recent call last):\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/rdd.py\", line 794, in func\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/unit.000000/leaflet-finder-parallel-cc.py\", line 20, in find_partial_connected_components\n    distances = (cdist(window[0],window[1])<cutoff)  # check indexes\n  File \"/home/iparask/radical.pilot.sandbox/ve_comet/lib/python2.7/site-packages/scipy/spatial/distance.py\", line 2037, in cdist\n    dm = np.zeros((mA, mB), dtype=np.double)\nMemoryError\n","Stack Trace":[{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"read","File Name":"PythonRDD.scala","Line Number":166},{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"<init>","File Name":"PythonRDD.scala","Line Number":207},{"Declaring Class":"org.apache.spark.api.python.PythonRunner","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":125},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":70},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":300},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":264},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":66},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":88},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":214},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1142},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":617},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":745}],"Full Stack Trace":"org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/rdd.py\", line 794, in func\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/unit.000000/leaflet-finder-parallel-cc.py\", line 20, in find_partial_connected_components\n    distances = (cdist(window[0],window[1])<cutoff)  # check indexes\n  File \"/home/iparask/radical.pilot.sandbox/ve_comet/lib/python2.7/site-packages/scipy/spatial/distance.py\", line 2037, in cdist\n    dm = np.zeros((mA, mB), dtype=np.double)\nMemoryError\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n","Metrics":{"Host Name":"198.202.113.161","Executor Deserialize Time":0,"Executor Run Time":318,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}},"Task Info":{"Task ID":69,"Index":36,"Attempt":1,"Launch Time":1487698436722,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1487698437063,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"198.202.113.161","Executor Deserialize Time":0,"Executor Run Time":318,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":72,"Index":36,"Attempt":2,"Launch Time":1487698437063,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"org.apache.spark.api.python.PythonException","Description":"Traceback (most recent call last):\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/rdd.py\", line 794, in func\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/unit.000000/leaflet-finder-parallel-cc.py\", line 20, in find_partial_connected_components\n    distances = (cdist(window[0],window[1])<cutoff)  # check indexes\n  File \"/home/iparask/radical.pilot.sandbox/ve_comet/lib/python2.7/site-packages/scipy/spatial/distance.py\", line 2037, in cdist\n    dm = np.zeros((mA, mB), dtype=np.double)\nMemoryError\n","Stack Trace":[{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"read","File Name":"PythonRDD.scala","Line Number":166},{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"<init>","File Name":"PythonRDD.scala","Line Number":207},{"Declaring Class":"org.apache.spark.api.python.PythonRunner","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":125},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":70},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":300},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":264},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":66},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":88},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":214},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1142},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":617},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":745}],"Full Stack Trace":"org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/rdd.py\", line 794, in func\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/unit.000000/leaflet-finder-parallel-cc.py\", line 20, in find_partial_connected_components\n    distances = (cdist(window[0],window[1])<cutoff)  # check indexes\n  File \"/home/iparask/radical.pilot.sandbox/ve_comet/lib/python2.7/site-packages/scipy/spatial/distance.py\", line 2037, in cdist\n    dm = np.zeros((mA, mB), dtype=np.double)\nMemoryError\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n","Metrics":{"Host Name":"198.202.113.161","Executor Deserialize Time":0,"Executor Run Time":327,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}},"Task Info":{"Task ID":70,"Index":2,"Attempt":2,"Launch Time":1487698436925,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1487698437277,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"198.202.113.161","Executor Deserialize Time":0,"Executor Run Time":327,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":73,"Index":2,"Attempt":3,"Launch Time":1487698437277,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"org.apache.spark.SparkException","Description":"Python worker exited unexpectedly (crashed)","Stack Trace":[{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"read","File Name":"PythonRDD.scala","Line Number":203},{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"<init>","File Name":"PythonRDD.scala","Line Number":207},{"Declaring Class":"org.apache.spark.api.python.PythonRunner","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":125},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":70},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":300},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":264},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":66},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":88},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":214},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1142},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":617},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":745}],"Full Stack Trace":"org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:203)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:139)\n\t... 11 more\n","Metrics":{"Host Name":"198.202.113.154","Executor Deserialize Time":0,"Executor Run Time":9275,"Result Size":0,"JVM GC Time":47,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}},"Task Info":{"Task ID":37,"Index":37,"Attempt":0,"Launch Time":1487698427823,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1487698437314,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"198.202.113.154","Executor Deserialize Time":0,"Executor Run Time":9275,"Result Size":0,"JVM GC Time":47,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":74,"Index":37,"Attempt":1,"Launch Time":1487698437315,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"org.apache.spark.api.python.PythonException","Description":"Traceback (most recent call last):\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/rdd.py\", line 794, in func\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/unit.000000/leaflet-finder-parallel-cc.py\", line 20, in find_partial_connected_components\n    distances = (cdist(window[0],window[1])<cutoff)  # check indexes\n  File \"/home/iparask/radical.pilot.sandbox/ve_comet/lib/python2.7/site-packages/scipy/spatial/distance.py\", line 2037, in cdist\n    dm = np.zeros((mA, mB), dtype=np.double)\nMemoryError\n","Stack Trace":[{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"read","File Name":"PythonRDD.scala","Line Number":166},{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"<init>","File Name":"PythonRDD.scala","Line Number":207},{"Declaring Class":"org.apache.spark.api.python.PythonRunner","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":125},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":70},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":300},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":264},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":66},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":88},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":214},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1142},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":617},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":745}],"Full Stack Trace":"org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/rdd.py\", line 794, in func\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/unit.000000/leaflet-finder-parallel-cc.py\", line 20, in find_partial_connected_components\n    distances = (cdist(window[0],window[1])<cutoff)  # check indexes\n  File \"/home/iparask/radical.pilot.sandbox/ve_comet/lib/python2.7/site-packages/scipy/spatial/distance.py\", line 2037, in cdist\n    dm = np.zeros((mA, mB), dtype=np.double)\nMemoryError\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n","Metrics":{"Host Name":"198.202.113.161","Executor Deserialize Time":0,"Executor Run Time":328,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}},"Task Info":{"Task ID":71,"Index":9,"Attempt":1,"Launch Time":1487698437029,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1487698437383,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"198.202.113.161","Executor Deserialize Time":0,"Executor Run Time":328,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":75,"Index":9,"Attempt":2,"Launch Time":1487698437383,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"org.apache.spark.SparkException","Description":"Python worker exited unexpectedly (crashed)","Stack Trace":[{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"read","File Name":"PythonRDD.scala","Line Number":203},{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"<init>","File Name":"PythonRDD.scala","Line Number":207},{"Declaring Class":"org.apache.spark.api.python.PythonRunner","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":125},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":70},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":300},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":264},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":66},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":88},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":214},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1142},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":617},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":745}],"Full Stack Trace":"org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:203)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:139)\n\t... 11 more\n","Metrics":{"Host Name":"198.202.113.152","Executor Deserialize Time":0,"Executor Run Time":9638,"Result Size":0,"JVM GC Time":53,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}},"Task Info":{"Task ID":4,"Index":4,"Attempt":0,"Launch Time":1487698427735,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1487698437631,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"198.202.113.152","Executor Deserialize Time":0,"Executor Run Time":9638,"Result Size":0,"JVM GC Time":53,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":76,"Index":4,"Attempt":1,"Launch Time":1487698437632,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"org.apache.spark.SparkException","Description":"Python worker exited unexpectedly (crashed)","Stack Trace":[{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"read","File Name":"PythonRDD.scala","Line Number":203},{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"<init>","File Name":"PythonRDD.scala","Line Number":207},{"Declaring Class":"org.apache.spark.api.python.PythonRunner","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":125},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":70},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":300},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":264},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":66},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":88},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":214},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1142},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":617},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":745}],"Full Stack Trace":"org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:203)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:139)\n\t... 11 more\n","Metrics":{"Host Name":"198.202.113.154","Executor Deserialize Time":0,"Executor Run Time":9976,"Result Size":0,"JVM GC Time":47,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}},"Task Info":{"Task ID":47,"Index":47,"Attempt":0,"Launch Time":1487698427838,"Executor ID":"0","Host":"198.202.113.154","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1487698438012,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"198.202.113.154","Executor Deserialize Time":0,"Executor Run Time":9976,"Result Size":0,"JVM GC Time":47,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":77,"Index":47,"Attempt":1,"Launch Time":1487698438012,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"org.apache.spark.SparkException","Description":"Python worker exited unexpectedly (crashed)","Stack Trace":[{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"read","File Name":"PythonRDD.scala","Line Number":203},{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"<init>","File Name":"PythonRDD.scala","Line Number":207},{"Declaring Class":"org.apache.spark.api.python.PythonRunner","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":125},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":70},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":300},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":264},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":66},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":88},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":214},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1142},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":617},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":745}],"Full Stack Trace":"org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:203)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:139)\n\t... 11 more\n","Metrics":{"Host Name":"198.202.113.152","Executor Deserialize Time":0,"Executor Run Time":10208,"Result Size":0,"JVM GC Time":53,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}},"Task Info":{"Task ID":20,"Index":20,"Attempt":0,"Launch Time":1487698427762,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1487698438200,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"198.202.113.152","Executor Deserialize Time":0,"Executor Run Time":10208,"Result Size":0,"JVM GC Time":53,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":78,"Index":20,"Attempt":1,"Launch Time":1487698438200,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"org.apache.spark.SparkException","Description":"Python worker exited unexpectedly (crashed)","Stack Trace":[{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"read","File Name":"PythonRDD.scala","Line Number":203},{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"<init>","File Name":"PythonRDD.scala","Line Number":207},{"Declaring Class":"org.apache.spark.api.python.PythonRunner","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":125},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":70},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":300},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":264},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":66},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":88},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":214},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1142},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":617},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":745}],"Full Stack Trace":"org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:203)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:139)\n\t... 11 more\n","Metrics":{"Host Name":"198.202.113.161","Executor Deserialize Time":0,"Executor Run Time":10137,"Result Size":0,"JVM GC Time":45,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}},"Task Info":{"Task ID":56,"Index":56,"Attempt":0,"Launch Time":1487698427882,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1487698438263,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"198.202.113.161","Executor Deserialize Time":0,"Executor Run Time":10137,"Result Size":0,"JVM GC Time":45,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":79,"Index":56,"Attempt":1,"Launch Time":1487698438264,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"org.apache.spark.api.python.PythonException","Description":"Traceback (most recent call last):\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/rdd.py\", line 794, in func\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/unit.000000/leaflet-finder-parallel-cc.py\", line 20, in find_partial_connected_components\n    distances = (cdist(window[0],window[1])<cutoff)  # check indexes\n  File \"/home/iparask/radical.pilot.sandbox/ve_comet/lib/python2.7/site-packages/scipy/spatial/distance.py\", line 2037, in cdist\n    dm = np.zeros((mA, mB), dtype=np.double)\nMemoryError\n","Stack Trace":[{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"read","File Name":"PythonRDD.scala","Line Number":166},{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"<init>","File Name":"PythonRDD.scala","Line Number":207},{"Declaring Class":"org.apache.spark.api.python.PythonRunner","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":125},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":70},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":300},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":264},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":66},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":88},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":214},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1142},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":617},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":745}],"Full Stack Trace":"org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/rdd.py\", line 794, in func\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/unit.000000/leaflet-finder-parallel-cc.py\", line 20, in find_partial_connected_components\n    distances = (cdist(window[0],window[1])<cutoff)  # check indexes\n  File \"/home/iparask/radical.pilot.sandbox/ve_comet/lib/python2.7/site-packages/scipy/spatial/distance.py\", line 2037, in cdist\n    dm = np.zeros((mA, mB), dtype=np.double)\nMemoryError\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n","Metrics":{"Host Name":"198.202.113.161","Executor Deserialize Time":0,"Executor Run Time":1022,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}},"Task Info":{"Task ID":73,"Index":2,"Attempt":3,"Launch Time":1487698437277,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1487698438318,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"198.202.113.161","Executor Deserialize Time":0,"Executor Run Time":1022,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"reduce at /home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/unit.000000/leaflet-finder-parallel-cc.py:79","Number of Tasks":64,"RDD Info":[{"RDD ID":1,"Name":"PythonRDD","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":64,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":64,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"","Submission Time":1487698426722,"Completion Time":1487698438325,"Failure Reason":"Job aborted due to stage failure: Task 2 in stage 0.0 failed 4 times, most recent failure: Lost task 2.3 in stage 0.0 (TID 73, 198.202.113.161): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/rdd.py\", line 794, in func\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/unit.000000/leaflet-finder-parallel-cc.py\", line 20, in find_partial_connected_components\n    distances = (cdist(window[0],window[1])<cutoff)  # check indexes\n  File \"/home/iparask/radical.pilot.sandbox/ve_comet/lib/python2.7/site-packages/scipy/spatial/distance.py\", line 2037, in cdist\n    dm = np.zeros((mA, mB), dtype=np.double)\nMemoryError\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:","Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":0,"Completion Time":1487698438330,"Job Result":{"Result":"JobFailed","Exception":{"Message":"Job aborted due to stage failure: Task 2 in stage 0.0 failed 4 times, most recent failure: Lost task 2.3 in stage 0.0 (TID 73, 198.202.113.161): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/rdd.py\", line 794, in func\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/unit.000000/leaflet-finder-parallel-cc.py\", line 20, in find_partial_connected_components\n    distances = (cdist(window[0],window[1])<cutoff)  # check indexes\n  File \"/home/iparask/radical.pilot.sandbox/ve_comet/lib/python2.7/site-packages/scipy/spatial/distance.py\", line 2037, in cdist\n    dm = np.zeros((mA, mB), dtype=np.double)\nMemoryError\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:","Stack Trace":[{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages","File Name":"DAGScheduler.scala","Line Number":1283},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1","Method Name":"apply","File Name":"DAGScheduler.scala","Line Number":1271},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1","Method Name":"apply","File Name":"DAGScheduler.scala","Line Number":1270},{"Declaring Class":"scala.collection.mutable.ResizableArray$class","Method Name":"foreach","File Name":"ResizableArray.scala","Line Number":59},{"Declaring Class":"scala.collection.mutable.ArrayBuffer","Method Name":"foreach","File Name":"ArrayBuffer.scala","Line Number":47},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"abortStage","File Name":"DAGScheduler.scala","Line Number":1270},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1","Method Name":"apply","File Name":"DAGScheduler.scala","Line Number":697},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1","Method Name":"apply","File Name":"DAGScheduler.scala","Line Number":697},{"Declaring Class":"scala.Option","Method Name":"foreach","File Name":"Option.scala","Line Number":236},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"handleTaskSetFailed","File Name":"DAGScheduler.scala","Line Number":697},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"doOnReceive","File Name":"DAGScheduler.scala","Line Number":1496},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"onReceive","File Name":"DAGScheduler.scala","Line Number":1458},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"onReceive","File Name":"DAGScheduler.scala","Line Number":1447},{"Declaring Class":"org.apache.spark.util.EventLoop$$anon$1","Method Name":"run","File Name":"EventLoop.scala","Line Number":48},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"runJob","File Name":"DAGScheduler.scala","Line Number":567},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":1824},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":1837},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":1850},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":1921},{"Declaring Class":"org.apache.spark.rdd.RDD$$anonfun$collect$1","Method Name":"apply","File Name":"RDD.scala","Line Number":909},{"Declaring Class":"org.apache.spark.rdd.RDDOperationScope$","Method Name":"withScope","File Name":"RDDOperationScope.scala","Line Number":147},{"Declaring Class":"org.apache.spark.rdd.RDDOperationScope$","Method Name":"withScope","File Name":"RDDOperationScope.scala","Line Number":108},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"withScope","File Name":"RDD.scala","Line Number":310},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"collect","File Name":"RDD.scala","Line Number":908},{"Declaring Class":"org.apache.spark.api.python.PythonRDD$","Method Name":"collectAndServe","File Name":"PythonRDD.scala","Line Number":405},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"collectAndServe","File Name":"PythonRDD.scala","Line Number":-1},{"Declaring Class":"sun.reflect.NativeMethodAccessorImpl","Method Name":"invoke0","File Name":"NativeMethodAccessorImpl.java","Line Number":-2},{"Declaring Class":"sun.reflect.NativeMethodAccessorImpl","Method Name":"invoke","File Name":"NativeMethodAccessorImpl.java","Line Number":62},{"Declaring Class":"sun.reflect.DelegatingMethodAccessorImpl","Method Name":"invoke","File Name":"DelegatingMethodAccessorImpl.java","Line Number":43},{"Declaring Class":"java.lang.reflect.Method","Method Name":"invoke","File Name":"Method.java","Line Number":497},{"Declaring Class":"py4j.reflection.MethodInvoker","Method Name":"invoke","File Name":"MethodInvoker.java","Line Number":231},{"Declaring Class":"py4j.reflection.ReflectionEngine","Method Name":"invoke","File Name":"ReflectionEngine.java","Line Number":379},{"Declaring Class":"py4j.Gateway","Method Name":"invoke","File Name":"Gateway.java","Line Number":259},{"Declaring Class":"py4j.commands.AbstractCommand","Method Name":"invokeMethod","File Name":"AbstractCommand.java","Line Number":133},{"Declaring Class":"py4j.commands.CallCommand","Method Name":"execute","File Name":"CallCommand.java","Line Number":79},{"Declaring Class":"py4j.GatewayConnection","Method Name":"run","File Name":"GatewayConnection.java","Line Number":207},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":745}]}}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"org.apache.spark.api.python.PythonException","Description":"Traceback (most recent call last):\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/rdd.py\", line 794, in func\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/unit.000000/leaflet-finder-parallel-cc.py\", line 20, in find_partial_connected_components\n    distances = (cdist(window[0],window[1])<cutoff)  # check indexes\n  File \"/home/iparask/radical.pilot.sandbox/ve_comet/lib/python2.7/site-packages/scipy/spatial/distance.py\", line 2037, in cdist\n    dm = np.zeros((mA, mB), dtype=np.double)\nMemoryError\n","Stack Trace":[{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"read","File Name":"PythonRDD.scala","Line Number":166},{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"<init>","File Name":"PythonRDD.scala","Line Number":207},{"Declaring Class":"org.apache.spark.api.python.PythonRunner","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":125},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":70},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":300},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":264},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":66},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":88},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":214},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1142},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":617},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":745}],"Full Stack Trace":"org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/rdd.py\", line 794, in func\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/unit.000000/leaflet-finder-parallel-cc.py\", line 20, in find_partial_connected_components\n    distances = (cdist(window[0],window[1])<cutoff)  # check indexes\n  File \"/home/iparask/radical.pilot.sandbox/ve_comet/lib/python2.7/site-packages/scipy/spatial/distance.py\", line 2037, in cdist\n    dm = np.zeros((mA, mB), dtype=np.double)\nMemoryError\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n","Metrics":{"Host Name":"198.202.113.152","Executor Deserialize Time":0,"Executor Run Time":2787,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}},"Task Info":{"Task ID":64,"Index":5,"Attempt":1,"Launch Time":1487698435523,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1487698438347,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"198.202.113.152","Executor Deserialize Time":0,"Executor Run Time":2787,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"org.apache.spark.api.python.PythonException","Description":"Traceback (most recent call last):\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/rdd.py\", line 794, in func\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/unit.000000/leaflet-finder-parallel-cc.py\", line 20, in find_partial_connected_components\n    distances = (cdist(window[0],window[1])<cutoff)  # check indexes\n  File \"/home/iparask/radical.pilot.sandbox/ve_comet/lib/python2.7/site-packages/scipy/spatial/distance.py\", line 2037, in cdist\n    dm = np.zeros((mA, mB), dtype=np.double)\nMemoryError\n","Stack Trace":[{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"read","File Name":"PythonRDD.scala","Line Number":166},{"Declaring Class":"org.apache.spark.api.python.PythonRunner$$anon$1","Method Name":"<init>","File Name":"PythonRDD.scala","Line Number":207},{"Declaring Class":"org.apache.spark.api.python.PythonRunner","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":125},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":70},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":300},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":264},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":66},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":88},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":214},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1142},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":617},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":745}],"Full Stack Trace":"org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/spark-1.5.2/python/lib/pyspark.zip/pyspark/rdd.py\", line 794, in func\n  File \"/home/iparask/radical.pilot.sandbox/spParCC_262K_32768_64_1-pilot.0000/unit.000000/leaflet-finder-parallel-cc.py\", line 20, in find_partial_connected_components\n    distances = (cdist(window[0],window[1])<cutoff)  # check indexes\n  File \"/home/iparask/radical.pilot.sandbox/ve_comet/lib/python2.7/site-packages/scipy/spatial/distance.py\", line 2037, in cdist\n    dm = np.zeros((mA, mB), dtype=np.double)\nMemoryError\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n","Metrics":{"Host Name":"198.202.113.152","Executor Deserialize Time":0,"Executor Run Time":2379,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}},"Task Info":{"Task ID":66,"Index":3,"Attempt":1,"Launch Time":1487698435937,"Executor ID":"1","Host":"198.202.113.152","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1487698438347,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"198.202.113.152","Executor Deserialize Time":0,"Executor Run Time":2379,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0}}
{"Event":"SparkListenerApplicationEnd","Timestamp":1487698438373}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"TaskKilled"},"Task Info":{"Task ID":75,"Index":9,"Attempt":2,"Launch Time":1487698437383,"Executor ID":"2","Host":"198.202.113.161","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1487698438483,"Failed":true,"Accumulables":[]}}
