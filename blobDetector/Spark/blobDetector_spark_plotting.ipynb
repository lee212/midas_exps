{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots of Blob Detector Algorithm on Comet Using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SparkLogToDataFrame(filename):\n",
    "    import json\n",
    "    import pandas\n",
    "    import numpy as np\n",
    "\n",
    "    columns=['StageID','TaskId','ExecutorID','LaunchTime','SchedulerDelay','DeserializeTime','Runtime','GettingResultTime','ResultSerializationTime','FinishTime','ResultSize']\n",
    "\n",
    "    stat_file=open(filename)\n",
    "    stats=list();\n",
    "    for line in stat_file:\n",
    "        stats.append(json.loads(line))\n",
    "\n",
    "    data=pandas.DataFrame(columns=columns,dtype=np.int64)\n",
    "    data_point = np.zeros((1,11),dtype=np.int64)\n",
    "    for stat in stats:\n",
    "        if stat['Event'] == 'SparkListenerTaskEnd':\n",
    "            data_point[0,0] = int(stat['Stage ID'])\n",
    "            data_point[0,1] = int(stat['Task Info']['Task ID'])\n",
    "            data_point[0,2] = int(stat['Task Info']['Executor ID'])\n",
    "            data_point[0,3] = int(stat['Task Info']['Launch Time'])\n",
    "            data_point[0,5] = int(stat['Task Metrics']['Executor Deserialize Time'])\n",
    "            data_point[0,6] = int(stat['Task Metrics']['Executor Run Time'])\n",
    "            data_point[0,7] = int(stat['Task Info']['Finish Time'])-int(stat['Task Info']['Getting Result Time']) if int(stat['Task Info']['Getting Result Time'])!=0 else 0 \n",
    "            data_point[0,8] = int(stat['Task Metrics']['Result Serialization Time'])\n",
    "            data_point[0,9] = int(stat['Task Info']['Finish Time'])\n",
    "            data_point[0,10] = int(stat['Task Metrics']['Result Size'])\n",
    "            \n",
    "            data_point[0,4]= data_point[0,9]-data_point[0,3]-data_point[0,5]-data_point[0,6]-data_point[0,8]-data_point[0,7]\n",
    "            data_case = pandas.DataFrame(data_point,columns=columns)\n",
    "            data = data.append(data_case,ignore_index=True)\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = glob.glob('*/app-*')\n",
    "plotting_df = pd.DataFrame(columns=['Nodes','Time_ms', 'Scaling'])\n",
    "for name in folders:\n",
    "    df = SparkLogToDataFrame(name)\n",
    "    time = (df['FinishTime'].max()-df['LaunchTime'].min()) / 1000.\n",
    "    node = int(re.findall('\\d+', name)[0])\n",
    "    if 'W' in name:\n",
    "        scale = 'W'\n",
    "    elif 'S' in name:\n",
    "        scale = 'S'\n",
    "    else:\n",
    "        scale = 'WS'\n",
    "    plotting_df.loc[len(plotting_df)]=[node, time, scale]\n",
    "    \n",
    "plotting_df.to_csv('spark_filtered_data_nonhomogeneous_images.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean and standard deviation of Spark times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iterations = 4\n",
    "tests = len(plotting_df)\n",
    "num_tests = tests / test_iterations \n",
    "\n",
    "times_spark = pd.DataFrame(columns=['nodes','time'])\n",
    "scaling = list()\n",
    "stderr = list()\n",
    "for i in range(int(num_tests)):\n",
    "    times_spark.loc[i] = plotting_df[i*test_iterations : (i+1)*test_iterations].mean().values\n",
    "    scaling.append(plotting_df['Scaling'][i*test_iterations])\n",
    "    stderr.append(plotting_df['Time_ms'][i*test_iterations : (i+1)*test_iterations].std()) \n",
    "    \n",
    "times_spark['scaling'] = scaling\n",
    "times_spark['stderr'] = stderr\n",
    "\n",
    "times_spark = times_spark.sort_values(by=['nodes']).set_index(['nodes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot mean Spark mean times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'times_spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-45a136e20b0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweak_mean\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mtimes_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes_spark\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scaling'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'W'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtimes_spark\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scaling'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'WS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scaling'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stderr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mweak_error\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtimes_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes_spark\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scaling'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'W'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtimes_spark\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scaling'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'WS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scaling'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stderr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstrong_mean\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtimes_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes_spark\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scaling'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'S'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtimes_spark\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scaling'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'WS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scaling'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stderr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstrong_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimes_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes_spark\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scaling'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'S'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtimes_spark\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scaling'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'WS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scaling'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stderr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'times_spark' is not defined"
     ]
    }
   ],
   "source": [
    "weak_mean    = times_spark.where((times_spark['scaling'] == 'W') | (times_spark['scaling'] == 'WS')).dropna().drop(columns=['scaling', 'stderr'])\n",
    "weak_error   = times_spark.where((times_spark['scaling'] == 'W') | (times_spark['scaling'] == 'WS')).dropna().drop(columns=['scaling', 'time'])['stderr']\n",
    "strong_mean  = times_spark.where((times_spark['scaling'] == 'S') | (times_spark['scaling'] == 'WS')).dropna().drop(columns=['scaling', 'stderr'])\n",
    "strong_error = times_spark.where((times_spark['scaling'] == 'S') | (times_spark['scaling'] == 'WS')).dropna().drop(columns=['scaling', 'time'])['stderr']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2,ncols=1,sharex=True,sharey=True)\n",
    "fig.set_size_inches(12,10)\n",
    "width = 0.2\n",
    "\n",
    "def merge_dicts(x, y):\n",
    "    z = x.copy()   # start with x's keys and values\n",
    "    z.update(y)    # modifies z with y's keys and values & returns None\n",
    "    return z\n",
    "\n",
    "shared_kwargs = {'kind'     : 'bar',\n",
    "                 'log'      : True,\n",
    "                 'fontsize' : 14\n",
    "                }\n",
    "\n",
    "weak_title    = {'title'    : 'Weak Scaling of Blob Detector Algorithm Using Spark (24 tasks per node)',\n",
    "                 'ax'       : axes[0],\n",
    "                 'yerr'     : weak_error\n",
    "                  }\n",
    "strong_title  = {'title'    : 'Strong Scaling of Blob Detector Algorithm Using Spark (192 tasks in total)',\n",
    "                 'ax'       : axes[1],\n",
    "                 'yerr'     : strong_error\n",
    "                }\n",
    "\n",
    "weak_kwargs     = merge_dicts(shared_kwargs, weak_title)\n",
    "weak_scale_plot = weak_mean.plot(**weak_kwargs)\n",
    "weak_scale_plot.set_ylabel('Duration (seconds)', fontsize=14)\n",
    "weak_scale_plot.set_yticks([1, 10, 100, 1000, 10000])\n",
    "weak_scale_plot.set_yticklabels(['1','10','100','1000', '10000'], fontsize=14)\n",
    "weak_scale_plot.grid('on', which='both', linestyle=':')\n",
    "axes[0].legend(['Time'])\n",
    "\n",
    "strong_kwargs     = merge_dicts(shared_kwargs, strong_title)\n",
    "strong_scale_plot = strong_mean.plot(**strong_kwargs)\n",
    "strong_scale_plot.set_ylabel('Duration (seconds)', fontsize=14)\n",
    "strong_scale_plot.set_yticks([1, 10, 100, 1000, 10000])\n",
    "strong_scale_plot.set_yticklabels(['1','10','100','1000', '10000'], fontsize=14)\n",
    "strong_scale_plot.set_xticklabels(['1','2','4','8'], rotation=0, fontsize=14)\n",
    "strong_scale_plot.grid('on', which='both', linestyle=':')\n",
    "strong_scale_plot.set_xlabel('Nodes', fontsize=14)\n",
    "axes[1].legend(['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
