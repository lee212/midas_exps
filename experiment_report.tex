%% LyX 2.1.4 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in,headheight=1in,headsep=1in,footskip=0.5in}
\usepackage{babel}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{pifont}
\usepackage[parfill]{parskip}
\newif\ifdraft
\drafttrue
\ifdraft
\usepackage{xcolor}
\definecolor{ocolor}{rgb}{1,0,0.4}
\newcommand{\onote}[1]{ {\textcolor{ocolor} { (***Ole: #1) }}}
\newcommand{\terminology}[1]{ {\textcolor{red} {(Terminology used: \textbf{#1}) }}}
\newcommand{\owave}[1]{ {\cyanuwave{#1}}}
\newcommand{\jwave}[1]{ {\reduwave{#1}}}
\newcommand{\alwave}[1]{ {\blueuwave{#1}}}
\newcommand{\jhanote}[1]{ {\textcolor{red} { ***shantenu: #1 }}}
\newcommand{\alnote}[1]{ {\textcolor{blue} { ***andreL: #1 }}}
\newcommand{\amnote}[1]{ {\textcolor{blue} { ***andreM: #1 }}}
\newcommand{\smnote}[1]{ {\textcolor{brown} { ***sharath: #1 }}}
\newcommand{\pmnote}[1]{ {\textcolor{brown} { ***Pradeep: #1 }}}
\newcommand{\msnote}[1]{ {\textcolor{cyan} { ***mark: #1 }}}
\newcommand{\mrnote}[1]{ {\textcolor{purple} { ***melissa: #1 }}}
\definecolor{orange}{rgb}{1,.5,0}
\newcommand{\aznote}[1]{ {\textcolor{orange} { ***ashley: #1 }}}
\definecolor{dandelion}{cmyk}{0,0.29,0.84,0}
\newcommand{\mtnote}[1]{ {\textcolor{dandelion} { ***matteo: #1 }}}
\newcommand{\gpnote}[1]{{\textcolor{green} {***giannis: #1}}}
\newcommand{\note}[1]{ {\textcolor{magenta} { ***Note: #1 }}}
\else
\newcommand{\onote}[1]{}
\newcommand{\terminology}[1]{}
\newcommand{\owave}[1]{#1}
\newcommand{\jwave}[1]{#1}
\newcommand{\alnote}[1]{}
\newcommand{\amnote}[1]{}
\newcommand{\athotanote}[1]{}
\newcommand{\smnote}[1]{}
\newcommand{\pmnote}[1]{}
\newcommand{\jhanote}[1]{}
\newcommand{\msnote}[1]{}
\newcommand{\mrnote}[1]{}
\newcommand{\aznote}[1]{}
\newcommand{\mtnote}[1]{}
\newcommand{\note}[1]{}
\fi
\begin{document}

\title{Experiments Report}

\maketitle

\begin{abstract}
This report will include the discussion for the experiments. The experiments section will have
data plotting and an initial analysis (model and discussion) based on the developed understanding. 
A Q \& A subsection will follow after the discussion. I will add questions there that still need answering.
It would be nice if others contributed with questions!
\end{abstract}

%\begin{itemize}
%	\item
%	\begin{itemize}
%		\item
%		\item
%		\item 
%		\item 
%		\item 
%		\item 
%		\item
%\end{itemize}


%\begnn{table}[t]
%\centering
%\begin{tabular}{|p{1.6cm}|p{1.6cm}|p{1.6cm}|p{1.6cm}|} \hline
%Property 				  &CPPTraj (RMSD) &Hausdorff Distance &Leaflet Finder\\ \hline
%Single Data Scan      	  &		   &                        & \\ \hline
%Iterative      			  &No 	   &No     &Connected Components \\ \hline5
%
%All-Pairs      			  &No      &Yes          &Pairwise Distance  \\ \hline
%Intermediate Data/Shuffle &Medium  &No Shuffling       & \\ \hline
%Output Data Volume        &Low	   &Low        & \\ \hline
%\end{tabular}
%\caption{Advanced Analytics Application Characteristics}
%\end{table}

%\begin{figure}[t]
%  \includegraphics[width=.45\textwidth]{figures/CPPTrajExecutionSchematicSingleTrajectory.jpg}\quad
%  \caption{}
%  \label{fig:}
%\end{figure}

\section*{CPPTraj RMSD}
The data reported are CPPTraj comparing experiments between Vanilla (MPI) execution and the task parallel execution of CPPTraj via RADICAL-Pilot. The experiments setup is the following:

\begin{itemize}
\item RMSD over 160000 frames as a single trajectory and as an ensemble of 2 trajectories that contain 80000 frames each. (105GB filesize)
\item RMSD over 320000 frames as a single trajectory and as an ensemble of 4 trajectories that contain 80000 frames each. (209GB filesize)
\item RMSD over 640000 frames as a single trajectory and as an ensemble of 8 trajectories that contain 80000 frames each. (418GB filesize)
\end{itemize}

The configuration was from a core per 80000 trajectories up to a node per 80000 frames. All experiments were done on Stampede.

\begin{figure}[H]
\centering
  \includegraphics[scale=.70]{cpptraj/cpptrajcomparison_160Kframes.pdf}
  \caption{Time to Execution and Throughput comparison between different ways of executing the same 
  CPPTraj analysis. There are in total 160K frames organized as a single trajectory file for the 
  Single trajectory case and as an ensemble of 2 trajectories for the ensemble case}
  \label{fig:cpptrajcomparison_160Kframes}
\end{figure}

\begin{figure}[H]
\centering
  \includegraphics[scale=.70]{cpptraj/cpptrajcomparison_320Kframes.pdf}
  \caption{Time to Execution and Throughput comparison between different ways of executing the same 
  CPPTraj analysis. There are in total 320K frames organized as a single trajectory file for the 
  Single trajectory case and as an ensemble of 4 trajectories for the ensemble case.}
  \label{fig:cpptrajcomparison_320Kframes}
\end{figure}

\begin{figure}[H]
\centering
  \includegraphics[scale=.70]{cpptraj/cpptrajcomparison_640Kframes.pdf}
  \caption{Time to Execution and Throughput comparison between different ways of executing the same 
  CPPTraj analysis. There are in total 640K frames organized as a single trajectory file for the 
  Single trajectory case and as an ensemble of 8 trajectories for the ensemble case.}
  \label{fig:cpptrajcomparison_640Kframes}
\end{figure}

The top subplot show the Execution time for Vanilla and RADICAL-Pilot. The bottom
subplot shows the Average Throughput. In all figures  the order of the bars is from 
right to left:
\begin{itemize}
\item[1] Single Trajectory Vanilla, 
\item[2] RP-CPPTraj single trajectory,
\item[3] CPPTraj Vanilla Ensemble and 
\item[4] RP-CPPTraj Ensemble
\end{itemize}  

One important note to make, is that as the core count increases, the MPI implementation does not scale in 
ensemble case as the task level parallel for the 320K, Figure \ref{fig:cpptrajcomparison_320Kframes}, and 
640K frames, Figure \ref{fig:cpptrajcomparison_640Kframes}. The main difference between those two is that 
the CPPTraj execution via RADICAL-Pilot introduces a small delay between the launching of each CPPTraj 
process. I believe that this delay reduces the strain CPPTraj's MPI implementation puts to the filesystem 
and the data are read faster. In the next set of experiments with RMSD, I want to find the filesize, or 
better the system size, where the MPI implementation cannot scale anymore and the task level parallel 
can.

The reason behind the above statement is the fact that throughput remains relatively stable. 
Throughput, here measured as frames per second, is the amount of computed data per time unit. We 
can say it is the computation velocity. Throughput is a function of input rate and the number
of computing blocks. By computating blocks, I mean a self contained element that takes an input, does 
some sort of processing on the input and gives an output. In this case,it can either be a MPI process 
or a task.

Assuming that the input rate, throughin, is infinite and it can feed continuously and steadily any number of 
computing blocks, the throughput will increase linearly as we increase the number of computing blocks.
Say that such a block can process $N$ inputs per time unit. Adding a second computing block $2N$ 
inputs per time unit can now be processed. Thus, with $K$ computing blocks the throughput is $KN$ inputs 
per time unit. It is now established how throughput changes when the computation blocks vary and the input rate
is large enough to accomodate any number of them. 

Assume now that the input rate is finite to a maximum of $M$ inputs per time unit. In case $M < N$, throughput
is dectated by the input rate. In case $M \geq N$, throughput will increase linearly as long as the number
of computing blocks is less or equal to $\lfloor\frac{M}{N}\rfloor$. When the number of computing blocks,
becomes larger than the previous number, throuput flats to a rate equal to the rate in which the input is produced.

The question that needs to be answered now is what is the rate that CPPTraj reads in data. The experiments will read
the file and do nothing else. 

\section*{Hausdorff Distance}

The data reported here are comparing the Hausdorff Distance calculation via a RADICAL-Pilot and a 
Spark implementation. Both use the same implementation for the main function and both use parallel
read. The experiments were executed over 192 trajectories of CA atoms on Comet. Figure 
\ref{fig:HausdorffMeanTaskBreakDownPr} show the mean time to execution for all three cases of 
trajectory size for the CA atoms. 

\begin{figure}[H]
\centering
  \includegraphics[scale=.70]{Hausdorff/MeanTaskBreakDownPr.pdf}
  \caption{Time to Execution Hausdorff Distance. From top to bottom Short trajectory, Medium 
           Trajectory, Long Trajectory}
  \label{fig:HausdorffMeanTaskBreakDownPr}
\end{figure}

Figures \ref{fig:DetailedMeanTaskBreakDownShort},\ref{fig:DetailedMeanTaskBreakDownMed} and 
\ref{fig:DetailedMeanTaskBreakDownLong} show in more detail how the execution between the two 
frameworks differ. Two main differences is that RADICAL-Pilot spends more time to stage in and 
stage out data to and from a task, where Spark is more expensive in scheduling tasks.

\begin{figure}[H]
\centering
  \includegraphics[scale=.70]{Hausdorff/DetailedMeanTaskBreakDownShortPr.pdf}
  \caption{Execution break down between RADICAL-Pilot and Spark execution. The Y axis represents 
  the time spent in each part and the X axis the number of core. This is short trajectory size}
  \label{fig:DetailedMeanTaskBreakDownShort}
\end{figure}

\begin{figure}[H]
\centering
  \includegraphics[scale=.70]{Hausdorff/DetailedMeanTaskBreakDownMedPr.pdf}
  \caption{Execution break down between RADICAL-Pilot and Spark execution. The Y axis represents 
  the time spent in each part and the X axis the number of core. This is medium trajectory size}
  \label{fig:DetailedMeanTaskBreakDownMed}
\end{figure}

\begin{figure}[H]
\centering
  \includegraphics[scale=.70]{Hausdorff/DetailedMeanTaskBreakDownLongPr.pdf}
  \caption{Execution break down between RADICAL-Pilot and Spark execution. The Y axis represents 
  the time spent in each part and the X axis the number of core. This is Long trajectory size}
  \label{fig:DetailedMeanTaskBreakDownLong}
\end{figure}

\subsection*{Useful Definitions}
\begin{itemize}
\item $N_I$: Number of Input files

\item $S_I$: Size of each file

\item $k$: Number of tasks

\item $N_O$: Number of Output files

\item $S_O$: Size of each file

\item $\alpha$: the coefficient of Staging In

\item $\beta$: the coefficient of the Scheduling delay

\item $\gamma$: the coefficient of the Execution

\item $\delta$: the coefficient of the Staging Out
\end{itemize}

\subsection*{Analysis}
The execution model can be easily broken to different parts.  First part of the model is data 
StageIn. In case of RADICAL-Pilot StageIn is rather easy to undeerstand. In case of Spark, I 
consider as StageIn the part of the code that is written before partitioning the data. Second part 
is the time need to schedule a task. Third is the actual execution of the task, which can be broken 
further more to read, exec and write. Finally, the last part of the model is the time necessary to 
stage out the data. In case of RADICAL-Pilot it is easy to understand. In Spark, I consider as the 
time needed from the time that all tasks have returned their data until the end of the script.

Essentially, the model will look like:

$$T=\alpha(N_I S_I)+\beta\frac{k(k+1)}{2}+\gamma Y+\delta\Big{(}N_O S_O + \frac{k(k+1)}{2}\Big{)}$$

$Y$ is the tme of the execution of the task.
\subsubsection*{Task Execution Analysis}
That is dependent to the number of trajectories being processed and the number of points in each 
trajectory. Let $T_N$ be the number of trajectories per task and $T_S$ the size of each trajectory, 
i.e. the number of points.Thus, the above execution time can be

$$Y = (T_N T_S)r+{T_N}^2 dH +{T_N}^2 w$$

Let $dH$ be the time to calculate the Hausdorff distance between two trajectories. The following 
algorithm describes it in pseudocode. The description will help the following analysis:

\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{HausdorffDistance}{$T_1,T_2$}\Comment{$T_1$ and $T_2$ are a set of 3D points}
\For{\forall $t_1$ in $T_1$}
\For{\forall$t_2$ in $T_2$}
\State \texttt{Append in $D_1$ calculated d($t_1, t_2$)}
\EndFor
\State\texttt{$D_t_1$ append max($D_1$)}
\EndFor
\State $N_1$ = min($D_t_1$)
\For{\forall $t_2$ in $T_2$}
\For{\forall$t_1$ in $T_1$}
\State \texttt{Append in $D_2$ calculated d($t_2, t_1$)}
\EndFor
\State\texttt{$D_t_2$ append max($D_2$)}
\EndFor
\State $N_2$ = min($D_t_2$)
\State \textbf{return} max($N_1$,$N_2$)
\EndProcedure
\end{algorithmic}
\end{algorithm}

Thus, the complexity of $dH$ is

$$dH =\mathcal{O}({T_S}^2) + T_S \mathcal{O}(T_S) + \mathcal{O}({T_S}^2) + T_S \mathcal{O}(T_S)$$

\subsection*{RP Fitting}
C is the coefficient vector. y is the total execution times and A will be the matrix that holds the 
several values. The calculations were done for CA short 21,36,78 and 136 tasks and the result is 
used to verify the rest.

$$y = \left[\begin{array}{cccc} 20.83152014 \\  12.18815162 \\  6.06983352  \\  3.94762929\end{array}\right]$$

$$ A = \left[\begin{array}{cccc}
    544 &  21 &   20.7635511 & 21 \\
    408 & 36  & 12.1358488  &  36  \\
    272 & 78  & 6.03011796  &  78  \\  
    204 & 136 & 3.91804321  &  136\end{array}\right]$$
$$C = \left[\begin{array}{cccc}1.34411980e-05 \\  9.88082352e+11 \\  9.99759905e-01 \\ -9.88082352e+11\end{array}\right]$$

The next table shows the calculated values for the time to execution, the actual measured mean values and the ablsolute error from the calculation.

\begin{table}[H]
\centering
\begin{tabular}{ccccc} \hline
Trajectory Size  &Number of Tasks & Predicted Time &Actual Time & Error\\ \hline
Short & 300 & 2.19505713407 & 2.38752645466 & 0.192469320591\\ \hline
Short & 528 & 1.26865175492 & 1.61307469131 & 0.344422936393\\ \hline
Medium & 21 & 33.3136668993 & 33.3253119711 & 0.0116450718086\\ \hline
Medium & 36 & 22.2328782699 & 22.2469520392 & 0.0140737693618\\ \hline
Medium & 78 & 10.8452443487 & 10.8571666795 & 0.0119223308157\\ \hline
Medium & 136 & 6.8738059856 & 6.88058625833 & 0.00678027273885\\ \hline
Medium & 300 & 3.63918139191 & 3.67618065543 & 0.0369992635193\\ \hline
Medium & 528 & 2.40475870352 & 3.23822765489 & 0.833468951372\\ \hline
Long & 21 & 43.8873638582 & 43.8923199669 & 0.00495610872072\\ \hline
Long & 36 & 30.9028212926 & 30.9349148759 & 0.0320935833527\\ \hline
Long & 78 & 17.1266483406 & 17.1459104994 & 0.0192621588231\\ \hline
Long & 136 & 11.1204478449 & 11.1351985826 & 0.0147507377432\\ \hline
Long & 300 & 4.34514286961 & 5.26987538991 & 0.924732520303\\ \hline
Long & 528 & 4.34051945651 & 5.26987538991 & 0.929355933395\\ \hline
\end{tabular}
\end{table}
\subsection*{Spark Fitting}

The Spark execution model requires one extra term. That term is shows the time that is needed to
for Spark to launch its executors. An executor is the process responsibe to execute the tasks. So,
the model will be 

$$T=\alpha(N_I S_I)+\beta\frac{k(k+1)}{2}+\gamma Y+\delta\Big{(}N_O S_O + \frac{k(k+1)}{2}\Big{)}+\epsilonW$$

where $W$ is the number of worker nodes.

\gpnote{TODO: Solve the system}

\end{document}
